'''@Time: 2019/12/30 14:47@Author: xshura@File: load_data.py@Mail: 2209032305@qq.com'''from __future__ import print_functionimport osimport pickle as pklimport sysimport networkx as nximport numpy as npimport scipy.sparse as spimport matplotlib.pyplot as pltimport mathimport randomdef parse_index_file(filename):    """Parse index file."""    index = []    for line in open(filename):        index.append(int(line.strip()))    return indexdef sample_mask(idx, l):    """Create mask."""    mask = np.zeros(l)    mask[idx] = 1    return np.array(mask, dtype=np.bool)def gauss(arange):    # 高斯    import numpy as np    import math    u = 1  # 均值μ    sig = math.sqrt(0.2)  # 标准差δ    x = np.linspace(u - 3 * sig, u + 3 * sig, arange)    y_sig = np.exp(-(x - u) ** 2 / (2 * sig ** 2)) / (math.sqrt(2 * math.pi) * sig)    return y_sigdef compute_num_edges_nodes(edges):    node_edges = {}    for e in edges:        if e[0] in node_edges:            node_edges[e[0]] = node_edges[e[0]] + 1        else:            node_edges[e[0]] = 1        # if e[1] in node_edges:        #     node_edges[e[0]] = node_edges[e[0]] + 1        # else:        #     node_edges[e[0]] = 1    degrees = sorted(node_edges.items(), key=lambda x: x[1])[::-1]    res = {}    for x, y in degrees:        res[x] = y    # y = [x[1] for x in node_edges.items()]    # plt.plot(y)    # # plt.hist(y, bins=30)    # # plt.xlim(0 , 40)    # plt.show()    return resdef sigmoid(x):    s = 1 / (1 + np.exp(-x))    return sdef entropy(c):    classes = {}    for x in c:        if x not in classes.keys():            classes[x] = 1        else:            classes[x] += 1    e = 0.0    for k in classes:        p = classes[k] / len(c)        e += (-p) * math.log(p, 2)    return edef get_node_entropy(edges, labels):    labels = labels.tolist()    node_edges = {}    for e in edges:        if e[1] < len(labels):            if e[0] in node_edges.keys():                if 1 in labels[e[1]]:                    temp = labels[e[1]].index(1)                    node_edges[e[0]].append(temp)            else:                if 1 in labels[e[1]]:                    node_edges[e[0]] = [labels[e[0]].index(1)]                    node_edges[e[0]].append(labels[e[1]].index(1))        # else:        #     # print(e)    for k, v in node_edges.items():        node_edges[k] = entropy(v)    nodes_entropy = sorted(node_edges.items(), key=lambda x: x[0])    res = {}    for x, y in nodes_entropy:        res[x] = y    return resdef get_new_entropy(g, labels):    labels = labels.tolist()    node_edges = {}    for k in g.nodes:        temp = []        if k < len(labels) and 1 in labels[k]:            temp = [labels[k].index(1)]        for x in g[k]:            if x < len(labels):                if 1 in labels[x]:                    temp.append(labels[x].index(1))            else:                if k < len(labels):                    temp.append(labels[k].index(1))        node_edges[k] = temp    for k, v in node_edges.items():        node_edges[k] = entropy(v)    nodes_entropy = sorted(node_edges.items(), key=lambda x: x[1])    res = {}    for x, y in nodes_entropy:        res[x] = y    return resdef analyze(node_entropy, node_degree):    # analyze the degree and entropy    degree = []    entropy = []    for n in list(node_entropy.keys()):        entropy.append(node_entropy[n]*40)        degree.append(node_degree[n])    plt.plot(np.array(entropy), label='entropy')    plt.plot(np.array(degree), label='degree')    plt.legend()    plt.show()def load_data_by_str(dataset_str,  score=None, alpha=None):    """Load data."""    DATA_PATH = 'data/all_data/'    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']    objects = []    for i in range(len(names)):        with open("{}ind.{}.{}".format(DATA_PATH, dataset_str, names[i]), 'rb') as f:            if sys.version_info > (3, 0):                objects.append(pkl.load(f, encoding='latin1'))            else:                objects.append(pkl.load(f))    x, y, tx, ty, allx, ally, graph = tuple(objects)    test_idx_reorder = parse_index_file("{}ind.{}.test.index".format(DATA_PATH, dataset_str))    test_idx_range = np.sort(test_idx_reorder)    if dataset_str == 'citeseer':        # Fix citeseer dataset (there are some isolated nodes in the graph)        # Find isolated nodes, add them as zero-vecs into the right position        test_idx_range_full = range(min(test_idx_reorder),                                    max(test_idx_reorder) + 1)        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))        tx_extended[test_idx_range - min(test_idx_range), :] = tx        tx = tx_extended        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))        ty_extended[test_idx_range - min(test_idx_range), :] = ty        ty = ty_extended    labels = np.vstack((ally, ty))    labels[test_idx_reorder, :] = labels[test_idx_range, :]    features = sp.vstack((allx, tx)).tolil()    features[test_idx_reorder, :] = features[test_idx_range, :]    g = nx.from_dict_of_lists(graph)    # calculate the degree of nodes    degree = g.degree    node_degree = {}    for k, v in degree:        node_degree[k] = v    edges = list(g.edges())    if type(score) != np.ndarray:        # predict the labels of test data for calculating the entropy of nodes        # preds_txt = np.loadtxt('{}_preds.txt'.format(dataset_str), dtype=np.float32)        # preds = np.zeros([preds_txt.shape[0], ty.shape[1]])        # for i in range(preds.shape[0]):        #     id = int(preds_txt[i])        #     preds[i, id] = 1        #        # node_entropy = get_node_entropy(edges, np.vstack([ally, preds]))        node_entropy = get_node_entropy(edges, labels)        score = np.array([1]*len(edges))        # node_entropy = get_new_entropy(g, np.vstack([ally, preds]))        if alpha:            for i, e in enumerate(edges):                if e[0] in node_entropy.keys():                    score[i] = (1 - node_entropy[e[0]]) + alpha                    # score[i] = max((1 - node_entropy[e[0]]) + alpha, score[i])                else:                    if e[1] in node_entropy.keys():                        score[i] = max((1 - node_entropy[e[0]]) + alpha, score[i])    # np.savetxt("{}_sec_score.txt".format(dataset_str), score, fmt='%f', delimiter=',')    # analyzing the relationship between entropy and degree in dataset    g.add_weighted_edges_from([(edges[i][0], edges[i][1], score[i]) for i in range(len(edges))])    adj = nx.adjacency_matrix(g)    idx_test = test_idx_range.tolist()    # idx_test = range(len(y)+500, len(y)+1500)    idx_train = range(len(y))    idx_val = range(len(y), len(y) + 500)    train_mask = sample_mask(idx_train, labels.shape[0])    val_mask = sample_mask(idx_val, labels.shape[0])    test_mask = sample_mask(idx_test, labels.shape[0])    y_train = np.zeros(labels.shape)    y_val = np.zeros(labels.shape)    y_test = np.zeros(labels.shape)    y_train[train_mask, :] = labels[train_mask, :]    y_val[val_mask, :] = labels[val_mask, :]    y_test[test_mask, :] = labels[test_mask, :]    return adj, features, y_train, y_val, y_test, idx_train, idx_val, idx_test, train_maskdef preprocess_features(features):    """Row-normalize feature matrix and convert to tuple representation"""    rowsum = np.array(features.sum(1))    r_inv = np.power(rowsum, -1).flatten()    r_inv[np.isinf(r_inv)] = 0.    r_mat_inv = sp.diags(r_inv)    features = r_mat_inv.dot(features)    return features.todense()